"""
M3Benchä»»åŠ¡ç”Ÿæˆå™¨ V2 - åŸºäºé…ç½®æ–‡ä»¶
==========================================

ä½¿ç”¨dataset_configs.yamlé©±åŠ¨çš„ä»»åŠ¡ç”Ÿæˆç³»ç»Ÿã€‚

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ä»é…ç½®æ–‡ä»¶åŠ è½½æ•°æ®é›†ä¿¡æ¯
2. ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆæ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡ç±»å‹
3. æ”¯æŒä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤
4. è‡ªåŠ¨ä¿å­˜ç”Ÿæˆç»“æœå’Œå…ƒæ•°æ®

æ”¯æŒçš„ä»»åŠ¡ç±»å‹ï¼š
- Attribute Bridge Reasoning (ABR): å¤šè·³å±æ€§æ¨ç†
- Attribute Comparison (AC): å±æ€§å¯¹æ¯” [NEW!]
- Visual Noise Filtering (VNF): è§†è§‰å™ªå£°è¿‡æ»¤
- Relation Comparison (RC): å…³ç³»å¯¹æ¯”
"""

import sys
from pathlib import Path
import logging
import json
import shutil
from datetime import datetime
from typing import Dict, List, Any

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from dataprovider import DataLoader, DataGeneratorV2, load_config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def find_next_run_number(base_dir="d:\\install_file\\M3Bench\\generated_tasks_v2"):
    """æ‰¾åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„ run ç¼–å·"""
    base_path = Path(base_dir)
    base_path.mkdir(exist_ok=True)

    existing_runs = [d for d in base_path.iterdir()
                    if d.is_dir() and d.name.startswith('run_')]

    if not existing_runs:
        return 1

    run_numbers = []
    for run_dir in existing_runs:
        try:
            num = int(run_dir.name.split('_')[1])
            run_numbers.append(num)
        except:
            continue

    return max(run_numbers) + 1 if run_numbers else 1


def setup_output_directory(base_dir="d:\\install_file\\M3Bench\\generated_tasks_v2"):
    """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
    run_number = find_next_run_number(base_dir)
    run_dir = Path(base_dir) / f"run_{run_number}"

    # åˆ›å»ºå­ç›®å½•
    (run_dir / "tasks").mkdir(parents=True, exist_ok=True)
    (run_dir / "images").mkdir(parents=True, exist_ok=True)
    (run_dir / "annotations").mkdir(parents=True, exist_ok=True)
    (run_dir / "logs").mkdir(parents=True, exist_ok=True)

    return run_dir, run_number


def copy_image_with_check(src_path, dst_dir):
    """å¤åˆ¶å›¾ç‰‡å¹¶éªŒè¯"""
    src = Path(src_path)

    if not src.exists():
        logger.debug(f"Image not found (will skip copy): {src}")
        # Return relative path even if original doesn't exist
        # This ensures consistent path format in output
        return f"images/{src.name}"

    dst = Path(dst_dir) / src.name

    try:
        if not dst.exists():  # é¿å…é‡å¤å¤åˆ¶
            shutil.copy2(src, dst)
        return f"images/{src.name}"
    except Exception as e:
        logger.error(f"Failed to copy {src}: {e}")
        # Return relative path even if copy fails
        return f"images/{src.name}"


def process_task_with_images(task, run_dir):
    """å¤„ç†å•ä¸ªä»»åŠ¡ï¼šå¤åˆ¶å›¾ç‰‡å’Œæ ‡æ³¨"""
    try:
        # å¤åˆ¶å›¾ç‰‡
        image_paths = task.get('images', [])
        new_image_paths = []
        missing_count = 0

        for img_path in image_paths:
            relative_path = copy_image_with_check(img_path, run_dir / "images")
            if relative_path:
                new_image_paths.append(relative_path)
                if not Path(img_path).exists():
                    missing_count += 1
            else:
                logger.debug(f"Skipping task {task.get('task_id')}: missing image {img_path}")
                # ä¸è¦å› ä¸ºå•ä¸ªå›¾ç‰‡ç¼ºå¤±è€Œèˆå¼ƒæ•´ä¸ªä»»åŠ¡
                new_image_paths.append(img_path)  # ä½¿ç”¨åŸå§‹è·¯å¾„
                missing_count += 1

        # ä¿å­˜æ¨ç†è¯æ®
        if 'reasoning_evidence' in task:
            annot_file = run_dir / "annotations" / f"{task['task_id']}_evidence.json"
            annot_file.parent.mkdir(parents=True, exist_ok=True)

            with open(annot_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'task_id': task['task_id'],
                    'evidence': task['reasoning_evidence'],
                    'saved_at': datetime.now().isoformat()
                }, f, indent=2, ensure_ascii=False)

            task['evidence_file'] = f"annotations/{annot_file.name}"

        # æ›´æ–°ä»»åŠ¡ä¸­çš„å›¾ç‰‡è·¯å¾„
        task['images'] = new_image_paths
        task['run_info'] = {
            'generated_at': datetime.now().isoformat(),
            'quality_verified': True,
            'image_files_copied': len(new_image_paths) - missing_count,
            'missing_images': missing_count
        }

        # ç§»é™¤åŸå§‹è¯æ®ï¼ˆå·²ä¿å­˜åˆ°å•ç‹¬æ–‡ä»¶ï¼‰
        if 'reasoning_evidence' in task:
            del task['reasoning_evidence']

        return task

    except Exception as e:
        logger.error(f"Failed to process task {task.get('task_id', 'unknown')}: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_dataset_tasks(generator: DataGeneratorV2,
                          dataset_id: str,
                          run_dir: Path,
                          num_samples: int = 10,
                          split: str = "train") -> Dict[str, List[Dict]]:
    """
    ä»æŒ‡å®šæ•°æ®é›†ç”Ÿæˆæ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡ã€‚

    Args:
        generator: DataGeneratorV2 å®ä¾‹
        dataset_id: æ•°æ®é›†ID (e.g., 'mscoco14', 'vcr')
        run_dir: è¾“å‡ºç›®å½•
        num_samples: æ¯ç§ä»»åŠ¡ç”Ÿæˆçš„æ ·æœ¬æ•°
        split: æ•°æ®split

    Returns:
        å­—å…¸: {task_type: [processed_tasks]}
    """
    logger.info(f"\n{'='*70}")
    logger.info(f"å¤„ç†æ•°æ®é›†: {dataset_id.upper()}")
    logger.info(f"{'='*70}")

    all_tasks = {}

    try:
        # ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡
        tasks_by_type = generator.generate_all_tasks_for_dataset(
            dataset_id=dataset_id,
            num_samples_per_task=num_samples,
            split=split
        )

        # å¤„ç†æ¯ç§ä»»åŠ¡ç±»å‹
        for task_type, tasks in tasks_by_type.items():
            logger.info(f"\nå¤„ç†ä»»åŠ¡ç±»å‹: {task_type}")
            logger.info(f"  åŸå§‹ç”Ÿæˆ: {len(tasks)} ä¸ªä»»åŠ¡")

            # å¤åˆ¶å›¾ç‰‡å’Œå¤„ç†ä»»åŠ¡
            processed = []
            for task in tasks:
                processed_task = process_task_with_images(task, run_dir)
                if processed_task:
                    processed.append(processed_task)

            if processed:
                # ä¿å­˜åˆ°JSONLæ–‡ä»¶ï¼Œä½¿ç”¨å®Œæ•´çš„ä»»åŠ¡ç±»å‹åç§°
                task_type_full = {
                    'VNF': 'visual_noise_filtering',
                    'ABR': 'attribute_bridge_reasoning',
                    'RC': 'relation_comparison',
                    'AC': 'attribute_comparison'
                }.get(task_type, task_type)
                output_file = run_dir / "tasks" / f"{task_type_full}_{dataset_id}.jsonl"
                with open(output_file, 'w', encoding='utf-8') as f:
                    for task in processed:
                        f.write(json.dumps(task, ensure_ascii=False) + '\n')

                all_tasks[task_type] = processed
                logger.info(f"  âœ“ æˆåŠŸä¿å­˜: {len(processed)} ä¸ªä»»åŠ¡ -> {output_file.name}")
            else:
                logger.warning(f"  âœ— æ²¡æœ‰æœ‰æ•ˆä»»åŠ¡")

    except Exception as e:
        logger.error(f"ç”Ÿæˆä»»åŠ¡å¤±è´¥ ({dataset_id}): {e}")
        import traceback
        traceback.print_exc()

    return all_tasks


def generate_report(run_dir: Path, all_results: Dict[str, Dict], run_number: int):
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""

    # ç»Ÿè®¡ä¿¡æ¯
    total_tasks = 0
    tasks_by_type = {}
    tasks_by_dataset = {}

    for dataset_id, tasks_dict in all_results.items():
        dataset_total = 0
        for task_type, tasks in tasks_dict.items():
            count = len(tasks)
            total_tasks += count
            dataset_total += count

            if task_type not in tasks_by_type:
                tasks_by_type[task_type] = 0
            tasks_by_type[task_type] += count

        tasks_by_dataset[dataset_id] = dataset_total

    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'run_number': run_number,
        'generated_at': datetime.now().isoformat(),
        'summary': {
            'total_tasks': total_tasks,
            'datasets_processed': list(all_results.keys()),
            'task_types': list(tasks_by_type.keys())
        },
        'tasks_by_type': tasks_by_type,
        'tasks_by_dataset': tasks_by_dataset,
        'detailed_counts': {
            dataset_id: {
                task_type: len(tasks)
                for task_type, tasks in tasks_dict.items()
            }
            for dataset_id, tasks_dict in all_results.items()
        },
        'output_structure': {
            'tasks_directory': 'tasks/',
            'images_directory': 'images/',
            'annotations_directory': 'annotations/',
            'logs_directory': 'logs/'
        }
    }

    # ä¿å­˜JSONæŠ¥å‘Š
    report_file = run_dir / "REPORT.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # åˆ›å»ºMarkdownæŠ¥å‘Š
    readme_file = run_dir / "README.md"
    with open(readme_file, 'w', encoding='utf-8') as f:
        f.write(f"# M3Bench ä»»åŠ¡ç”ŸæˆæŠ¥å‘Š - Run {run_number}\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['generated_at']}\n\n")

        f.write("## ğŸ“Š æ€»è§ˆ\n\n")
        f.write(f"- **æ€»ä»»åŠ¡æ•°**: {total_tasks}\n")
        f.write(f"- **æ•°æ®é›†æ•°**: {len(all_results)}\n")
        f.write(f"- **ä»»åŠ¡ç±»å‹**: {', '.join(tasks_by_type.keys())}\n\n")

        f.write("## ğŸ“ˆ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡\n\n")
        for task_type, count in sorted(tasks_by_type.items()):
            f.write(f"- **{task_type}**: {count} ä¸ªä»»åŠ¡\n")

        f.write("\n## ğŸ“ æŒ‰æ•°æ®é›†ç»Ÿè®¡\n\n")
        for dataset_id, count in sorted(tasks_by_dataset.items()):
            f.write(f"### {dataset_id} ({count} ä¸ªä»»åŠ¡)\n\n")
            if dataset_id in all_results:
                for task_type, tasks in all_results[dataset_id].items():
                    f.write(f"  - {task_type}: {len(tasks)}\n")
                f.write("\n")

        f.write("## ğŸ“‚ ç›®å½•ç»“æ„\n\n")
        f.write("```\n")
        f.write(f"run_{run_number}/\n")
        f.write("â”œâ”€â”€ tasks/               # ç”Ÿæˆçš„ä»»åŠ¡æ–‡ä»¶ (JSONL)\n")
        f.write("â”œâ”€â”€ images/              # å¤åˆ¶çš„å›¾ç‰‡\n")
        f.write("â”œâ”€â”€ annotations/         # æ¨ç†è¯æ®å’ŒåŸå§‹æ ‡æ³¨\n")
        f.write("â”œâ”€â”€ logs/                # ç”Ÿæˆæ—¥å¿—\n")
        f.write("â”œâ”€â”€ REPORT.json          # JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š\n")
        f.write("â””â”€â”€ README.md            # æœ¬æ–‡ä»¶\n")
        f.write("```\n\n")

        f.write("## ğŸ” ä»»åŠ¡æ–‡ä»¶è¯´æ˜\n\n")
        f.write("æ¯ä¸ªä»»åŠ¡æ–‡ä»¶çš„æ ¼å¼ä¸º `{task_type}_{dataset_id}.jsonl`\n\n")
        f.write("ä»»åŠ¡å­—æ®µè¯´æ˜ï¼š\n")
        f.write("- `task_id`: ä»»åŠ¡å”¯ä¸€æ ‡è¯†\n")
        f.write("- `task_type`: ä»»åŠ¡ç±»å‹\n")
        f.write("- `images`: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰\n")
        f.write("- `question`: é—®é¢˜\n")
        f.write("- `answer`: ç­”æ¡ˆ\n")
        f.write("- `reasoning_depth`: æ¨ç†æ·±åº¦\n")
        f.write("- `evidence_file`: æ¨ç†è¯æ®æ–‡ä»¶è·¯å¾„\n")
        f.write("- `metadata`: å…ƒæ•°æ®\n")

    logger.info(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file} å’Œ {readme_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("M3Bench ä»»åŠ¡ç”Ÿæˆå™¨ V2 (é…ç½®é©±åŠ¨)")
    print("="*80)
    print("\næ”¯æŒçš„ä»»åŠ¡ç±»å‹:")
    print("  1. Attribute Bridge Reasoning (ABR)")
    print("  2. Attribute Comparison (AC) [NEW!]")
    print("  3. Visual Noise Filtering (VNF)")
    print("  4. Relation Comparison (RC)")
    print("\n" + "="*80 + "\n")

    # è®¾ç½®è¾“å‡ºç›®å½•
    run_dir, run_number = setup_output_directory()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {run_dir}")
    print(f"ğŸ”¢ è¿è¡Œç¼–å·: {run_number}\n")

    # è®¾ç½®æ—¥å¿—æ–‡ä»¶
    log_file = run_dir / "logs" / "generation.log"
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))
    logging.getLogger().addHandler(file_handler)

    try:
        # åŠ è½½é…ç½®
        logger.info("åŠ è½½é…ç½®æ–‡ä»¶...")
        import os
        # ä½¿ç”¨ç»å¯¹è·¯å¾„æŒ‡å®šé…ç½®æ–‡ä»¶
        config_file = "d:\\install_file\\M3Bench\\M3Bench-delivery\\dataprovider\\dataset_configs.yaml"
        config = load_config(config_file)

        # éªŒè¯æ•°æ®é›†è·¯å¾„
        logger.info("éªŒè¯æ•°æ®é›†è·¯å¾„...")
        path_validation = config.validate_dataset_paths()
        valid_datasets = [ds for ds, valid in path_validation.items() if valid]

        print("\nå¯ç”¨æ•°æ®é›†:")
        for dataset_id in valid_datasets:
            dataset_config = config.get_dataset_config(dataset_id)
            # æ˜¾ç¤ºæ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡ï¼Œè€Œä¸ä»…ä»…æ˜¯è¢«å¯ç”¨çš„ä»»åŠ¡
            supported_tasks = dataset_config.supported_tasks
            print(f"  âœ“ {dataset_id}: {', '.join(supported_tasks)}")
        
        # ç¡®ä¿åŒ…å«vcræ•°æ®é›†
        if 'vcr' not in valid_datasets:
            logger.warning("vcræ•°æ®é›†æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        logger.info("åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨...")
        import os
        # ä½¿ç”¨æ–°çš„æ•°æ®é›†è·¯å¾„
        data_root = "d:\install_file\M3Bench\dataset"
        loader = DataLoader(data_root=data_root)
        # ä½¿ç”¨ä¸è·¯å¾„éªŒè¯ç›¸åŒçš„é…ç½®æ–‡ä»¶
        config_file = "d:\install_file\M3Bench\M3Bench-delivery\dataprovider\dataset_configs.yaml"
        generator = DataGeneratorV2(loader, config_file=config_file)

        # ç”Ÿæˆä»»åŠ¡
        all_results = {}

        # é…ç½®ï¼šä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆå¤šå°‘æ ·æœ¬
        generation_config = {
            'mscoco14': {
                'num_samples': 5,
                'split': 'val'  # Use val split since images are in val2014 directory
            },
            'vcr': {
                'num_samples': 5,
                'split': 'train'
            },
            'scienceqa': {
                'num_samples': 5,
                'split': 'validation'
            },
            'docvqa': {
                'num_samples': 5,
                'split': 'validation'
            },
            'realworldqa': {
                'num_samples': 5,
                'split': 'test'
            }
        }

        for dataset_id in valid_datasets:
            if dataset_id not in generation_config:
                logger.info(f"è·³è¿‡ {dataset_id} (æœªé…ç½®)")
                continue

            config_for_dataset = generation_config[dataset_id]

            tasks_dict = generate_dataset_tasks(
                generator=generator,
                dataset_id=dataset_id,
                run_dir=run_dir,
                num_samples=config_for_dataset['num_samples'],
                split=config_for_dataset['split']
            )

            if tasks_dict:
                all_results[dataset_id] = tasks_dict

        # ç”ŸæˆæŠ¥å‘Š
        if all_results:
            logger.info("\nç”ŸæˆæŠ¥å‘Š...")
            generate_report(run_dir, all_results, run_number)

        # æ€»ç»“
        print("\n" + "="*80)
        print("âœ… ç”Ÿæˆå®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“‚ è¾“å‡ºä½ç½®: {run_dir}")

        if all_results:
            total_tasks = sum(
                len(tasks)
                for tasks_dict in all_results.values()
                for tasks in tasks_dict.values()
            )

            print(f"\nğŸ“Š ä»»åŠ¡ç»Ÿè®¡:")
            print(f"  æ€»è®¡: {total_tasks} ä¸ªä»»åŠ¡")

            for dataset_id, tasks_dict in all_results.items():
                dataset_total = sum(len(tasks) for tasks in tasks_dict.values())
                print(f"\n  {dataset_id}: {dataset_total} ä¸ªä»»åŠ¡")
                for task_type, tasks in tasks_dict.items():
                    print(f"    - {task_type}: {len(tasks)}")

            total_images = len(list((run_dir / 'images').glob('*')))
            total_annotations = len(list((run_dir / 'annotations').glob('*')))

            print(f"\nğŸ“ æ–‡ä»¶ç»Ÿè®¡:")
            print(f"  - å›¾ç‰‡: {total_images}")
            print(f"  - æ ‡æ³¨: {total_annotations}")
        else:
            print("\nâš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•ä»»åŠ¡")

        print(f"\nğŸ“„ æŸ¥çœ‹æŠ¥å‘Š:")
        print(f"  cat {run_dir / 'README.md'}")
        print()

    except Exception as e:
        logger.error(f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print(f"\nâŒ é”™è¯¯: {e}")
        print("è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦æƒ…")


if __name__ == "__main__":
    main()
